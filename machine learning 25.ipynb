{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a6dc65-8f83-45c0-af95-c951d019043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1:\n",
    "# The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for classification and regression tasks.\n",
    "# KNN is often used for pattern recognition and data mining applications.\n",
    "# the algorithm identifies the K nearest neighbors of a given data point in the feature space and assigns the majority class label or \n",
    "# calculates the average value (for regression) of those K neighbors to predict the label or value of the new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c647c21-c861-4b50-a1ee-86b14b2225e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2:\n",
    "# Choosing the value of K in the K-Nearest Neighbors (KNN) algorithm is an important decision that can impact the \n",
    "# performance and effectiveness of the algorithm. There is no one-size-fits-all answer for determining the optimal value of K, \n",
    "# as it depends on the characteristics of the data and the specific problem at hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893abade-6fa7-4310-b95e-78d7d88348d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3:\n",
    "# KNN Classifier\n",
    "# The KNN classifier is used for classification tasks where the goal is to assign a class label to a given input data point. \n",
    "# It determines the class label of a new data point based on the majority class labels of its K nearest neighbors. \n",
    "\n",
    "# KNN Regressor\n",
    "# The KNN regressor, on the other hand, is used for regression tasks where the goal is to predict a \n",
    "# continuous numerical value or quantity for a given input data point. Instead of predicting a class label, \n",
    "#  the KNN regressor predicts the value of the target variable by calculating the average of the target variable values among its K nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f581addb-9f77-4759-9715-8c5cf567461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4:\n",
    "# We can measure the performance of KNN by following metrics\n",
    "# 1.Accuracy: It measures the proportion of correctly classified instances to the total number of instances in the dataset. \n",
    "# 2.Precision, Recall, and F1-score: These metrics are commonly used for imbalanced datasets, where the class distribution is skewed.\n",
    "\n",
    "# Mean Squared Error (MSE): It measures the average squared difference between the predicted values and the actual values.\n",
    "# R-squared : It measures the proportion of the variance in the target variable that can be explained by the predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8539d435-8b50-453a-98d3-2446cf9aeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5:\n",
    "# The curse of dimensionality refers to a phenomenon that occurs when working with high-dimensional data in machine learning algorithms\n",
    "# 1.Increased sparsity:This means that the data points tend to be farther apart from each other, \n",
    "#   making it difficult to find meaningful patterns or neighbors for a given query point.\n",
    "# 2.Overfitting: The curse of dimensionality makes it easier for a model to overfit the training data. \n",
    "# 3.Accuracy : as we increases the features for model , model tend to give low accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e677794-7f3c-4507-b135-230217d01f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6:\n",
    "# 1.Deletion: If the dataset has missing values, one straightforward approach is to remove the instances (rows) that contain missing values.\n",
    "# 2.Imputation\n",
    "# Mean or median imputation: Replace missing values with the mean or median value of the feature. \n",
    "# Mode imputation: For categorical features, missing values can be imputed with the most frequent category (mode) of that feature.\n",
    "# KNN imputation: In this approach, the missing values are estimated by finding the nearest neighbors of the instance with missing values\n",
    "#   and using their values to impute the missing ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630bf489-3547-4672-b3b1-7af9b0afbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 7:\n",
    "# # KNN Classifier\n",
    "# The KNN classifier is used for classification tasks where the goal is to assign a class label to a given input data point. \n",
    "# It determines the class label of a new data point based on the majority class labels of its K nearest neighbors. \n",
    "# Problem Type: It is suitable for classification problems where the target variable is categorical.\n",
    "\n",
    "# # KNN Regressor\n",
    "# The KNN regressor, on the other hand, is used for regression tasks where the goal is to predict a \n",
    "# continuous numerical value or quantity for a given input data point. Instead of predicting a class label, \n",
    "# the KNN regressor predicts the value of the target variable by calculating the average of the target variable values among its K nearest neighbors.\n",
    "# Problem Type: It is suitable for regression problems where the target variable is continuous or numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d28f7dc-41e7-46f6-a7a7-c84c6c471f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 8:\n",
    "# Strenghts\n",
    "# 1.KNN is straightforward to understand and implement. It does not make strong assumptions about the underlying data distribution.\n",
    "# 2.KNN is a non-parametric algorithm, meaning it does not assume any specific functional form for the \n",
    "#   relationship between features and the target variable.\n",
    "# KNN can be used for both classification and regression tasks, making it versatile in various scenarios.\n",
    "\n",
    "# Weakness\n",
    "# 1.The main drawback of KNN is its computational complexity. \n",
    "#  As the dataset grows larger, the algorithm becomes slower and requires more memory to store the training data.\n",
    "# 2.The performance of KNN heavily depends on the choice of the parameter K, which represents the number of nearest neighbors to consider. \n",
    "\n",
    "# we can overcome these limitations\n",
    "# 1.By reducing the number of dimensions in the dataset through techniques like Principal Component Analysis (PCA), \n",
    "#   the computational complexity can be reduced.\n",
    "# 2.Conduct feature selection techniques such as correlation analysis, mutual information, or \n",
    "#   regularization to select the most relevant features and eliminate irrelevant ones.\n",
    "# 3.Use techniques like cross-validation to find the optimal value of K and other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54c640a-49ad-44d8-8929-8644909f8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 9:\n",
    "# # The main difference between the Euclidean distance metric and the Manhattan distance metric lies in the \n",
    "#  way they measure the distance between two points in a multi-dimensional space.\n",
    "\n",
    "# Euclidean Distance\n",
    "# The Euclidean distance between points (x1, y1) and (x2, y2) is given by the formula: \n",
    "# âˆš((x2 - x1)^2 + (y2 - y1)^2). In higher dimensions, the formula extends similarly.\n",
    "\n",
    "# Manhattan Distance\n",
    "#  In a 2-dimensional space, the Manhattan distance between points (x1, y1) and (x2, y2) is given by the formula: \n",
    "#  |x2 - x1| + |y2 - y1|. Again, this extends to higher dimensions in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214253d-d50e-4c76-a124-d6fbba5b13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 10:\n",
    "# n KNN, distances between data points are used to determine the \"closeness\" of neighbors.\n",
    "# If the features have different scales or units, the feature with larger values might dominate the distance calculation, \n",
    "# overpowering the contribution of other features. Scaling the features ensures that they have a similar range, \n",
    "# allowing each feature to have a balanced impact on the distance calculation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
