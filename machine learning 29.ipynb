{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e55786-43fa-4488-ac1c-dfa6f84fe920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1:\n",
    "# In the context of dimensionality reduction, a projection refers to the transformation of data from a higher-dimensional space to a \n",
    "# lower-dimensional space while preserving certain properties or relationships\n",
    "# Principal Component Analysis (PCA) is a widely used dimensionality reduction technique that utilizes projections. \n",
    "# In PCA, the goal is to find a set of orthogonal axes, known as principal components, onto which the data is projected.\n",
    "\n",
    "# It is used in PCA \n",
    "# 1.Compute the covariance matrix: First, the covariance matrix of the original data is calculated.\n",
    "# 2.Eigenvalue decomposition: The covariance matrix is then decomposed into its eigenvectors and eigenvalues. \n",
    "# 3.Selecting the desired number of components: Based on the desired dimensionality reduction, a subset of the eigenvectors is chosen.\n",
    "# 4.Projecting the data: The original data is projected onto the subspace spanned by the selected principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef86545b-469a-41d3-957a-05134e3596ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2:\n",
    "# The optimization problem in Principal Component Analysis (PCA) aims to find the optimal set of principal \n",
    "# components that capture the maximum amount of variance in the data. \n",
    "# PCA is framed to get the optimal components by computing these :\n",
    "# 1.Covariance matrix computation: The first step is to compute the covariance matrix of the original data\n",
    "# 2.Eigenvalue decomposition: The covariance matrix is then decomposed into its eigenvectors and eigenvalues. \n",
    "# 3.Selecting the desired number of components: Based on the desired dimensionality reduction, a subset of the eigenvectors is chosen.\n",
    "# 4.Projecting the data: The original data is projected onto the subspace spanned by the selected principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745d82b7-9ca8-445e-b72c-2283eef76a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3:\n",
    "# The relationship between covariance matrices and Principal Component Analysis (PCA) is \n",
    "# 1.The covariance matrix captures the relationships and dependencies between different dimensions (variables) of a dataset.\n",
    "# 2 . It provides information about how each variable varies in relation to others. \n",
    "# 3. In PCA, the covariance matrix is utilized to identify the principal component.\n",
    "# the covariance matrix provides the necessary information to identify the principal components in PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c85d81-034e-4c0c-88a0-931ffde77a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4:\n",
    "# choice of the number of principal components can impact PCA\n",
    "# 1.Capturing variance: The number of principal components determines the amount of variance captured from the original data.\n",
    "# 2.Dimensionality reduction: PCA is commonly used as a dimensionality reduction technique. \n",
    "#   Choosing a smaller number of principal components results in a more aggressive reduction in dimensionality.\n",
    "# 3.accuracy: PCA allows for the reconstruction of the original data from the reduced-dimensional representation. \n",
    "#   The number of principal components chosen influences the accuracy of the reconstructed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28705439-a345-4d52-82a3-56db432f85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5:\n",
    "# PCA can be used for feature selection, although it is primarily known as a dimensionality reduction technique\n",
    "# 1.Dimensionality reduction: PCA inherently reduces the dimensionality of the data by selecting a subset of the most important principal components. \n",
    "# By selecting a smaller number of principal components, PCA effectively performs feature selection by discarding the less informative features.\n",
    "\n",
    "# Benefits\n",
    "# 1.Unsupervised feature selection: PCA does not rely on class labels or target information, making it an unsupervised feature selection method.\n",
    "# 2.Reducing multicollinearity: Multicollinearity refers to the presence of strong correlations between features. \n",
    "#  It can negatively impact the performance and interpretability of machine learning models. \n",
    "#  PCA can help mitigate multicollinearity by transforming the original features into a set of orthogonal principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b55325e-7b46-4010-897b-c31f72759d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6:\n",
    "# Applications of PCA\n",
    "# 1.Dimensionality reduction: PCA is primarily used for dimensionality reduction.\n",
    "#  It helps in reducing the number of features or variables in a dataset while preserving the most important information or patterns.\n",
    "# 2.Data visualization: PCA is often employed for data visualization purposes. By projecting high-dimensional data onto a lower-dimensional space \n",
    "#  typically two or three dimensions, PCA allows for visual exploration and understanding of complex datasets\n",
    "# 3.Data visualization: PCA is often employed for data visualization purposes. By projecting high-dimensional data onto a lower-dimensional space, \n",
    "#  typically two or three dimensions, PCA allows for visual exploration and understanding of complex datasets\n",
    "# 4.Feature extraction: PCA can extract a set of representative features from high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a047dda5-cda7-4de5-90d0-22fb1d399a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 7\n",
    "# spread and are directly connected\n",
    "# Spread of the data: Spread refers to the extent or range of values that a dataset occupies along different dimensions. \n",
    "# It describes how the data points are distributed in the feature space. Spread can be visualized as the dispersion or scatter of data points.\n",
    "\n",
    "# Variance explained by principal components: In PCA, the principal components represent the directions of maximum variance in the data. \n",
    "# Each principal component captures a certain amount of variance in the data.\n",
    "\n",
    "# Principal components associated with higher variances capture more spread or variability in the data, \n",
    "# while components with lower variances capture less spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccb2492b-5aea-495a-b0d1-fe59f61ca37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 8:\n",
    "# PCA utilizes the spread and variance of the data to identify principal components,\n",
    "# PCA uses spread and variance in the process of identifying principal components:\n",
    "# 1.Covariance matrix computation: The first step is to compute the covariance matrix of the original data\n",
    "# 2.Eigenvalue decomposition: The covariance matrix is then decomposed into its eigenvectors and eigenvalues. \n",
    "# 3.Selecting the desired number of components: Based on the desired dimensionality reduction, a subset of the eigenvectors is chosen.\n",
    "# 4.Projecting the data: The original data is projected onto the subspace spanned by the selected principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e617d488-1eda-437e-9509-1041e2b56799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 9:\n",
    "# PCA handles data with high variance in some dimensions and low variance in others by identifying and capturing the directions of maximum variance, \n",
    "# regardless of whether the variance is high or low in specific dimensions.\n",
    "\n",
    "# 1.Equal importance of dimensions: PCA treats all dimensions or variables equally during the analysis. \n",
    "# It does not prioritize dimensions based on their individual variances. \n",
    "# Instead, PCA focuses on capturing the directions of maximum overall variance in the data, \n",
    "# irrespective of whether the variance is high or low in specific dimensions.\n",
    "# 2.important patterns: Even if certain dimensions have low variances, \n",
    "# they can still contribute to capturing important patterns or relationships when combined with other dimensions.\n",
    "# 3.Dimensionality importance: The importance of dimensions with low variances can still be reflected \n",
    "#   in the cumulative variance explained by the selected principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8581601-f6f3-4be5-8788-d810b9add242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
