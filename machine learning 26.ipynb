{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a332bb2-f8de-43a8-b181-177561c5a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1:\n",
    "# The main difference between the Euclidean distance metric and the Manhattan distance metric lies in the \n",
    "#  way they measure the distance between two points in a multi-dimensional space.\n",
    "\n",
    "# Euclidean Distance\n",
    "# The Euclidean distance between points (x1, y1) and (x2, y2) is given by the formula: \n",
    "# âˆš((x2 - x1)^2 + (y2 - y1)^2). In higher dimensions, the formula extends similarly.\n",
    "\n",
    "# Manhattan Distance\n",
    "#  In a 2-dimensional space, the Manhattan distance between points (x1, y1) and (x2, y2) is given by the formula: \n",
    "#  |x2 - x1| + |y2 - y1|. Again, this extends to higher dimensions in a similar manner.\n",
    "\n",
    "# following points which are to be taken in count\n",
    "# 1.Sensitivity to scale: Euclidean distance takes into account both the magnitude and direction of the differences between feature values. \n",
    "# 2.Influence of outliers: The Manhattan distance is less affected by outliers compared to the Euclidean distance. \n",
    "#   Since the Euclidean distance considers the squared differences, outliers with extreme values can have a larger impact on the distance calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952c9de5-60f5-4f62-87aa-10881a1c9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2:\n",
    "# some techniques to choose the  optimal k value:\n",
    "# 1.Cross-Validation: One commonly used technique is k-fold cross-validation. In this approach, the dataset is divided into k subsets or folds\n",
    "# 2.Grid Search: Grid search is another method where a predefined range of k values is specified, \n",
    "#   and the model's performance is evaluated for each value in the range.\n",
    "# 3.Domain Knowledge: Sometimes, prior knowledge about the dataset or problem domain can provide insights into choosing an appropriate value of k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1424d71-6158-4e4c-b357-0ac8cb76f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3:\n",
    "# Distance metric in a KNN classifier or regressor can significantly impact its performance. \n",
    "# The distance metric determines how the \"closeness\" between data points is measured.\n",
    "# There are 2 distance metrics\n",
    "# 1.Euclidean Distance: Euclidean distance is the most commonly used distance metric.\n",
    "#   It measures the straight-line distance between two points in the feature space.\n",
    "# 2.Manhattan Distance: Manhattan distance, also known as city block distance or L1 distance, \n",
    "#   calculates the sum of absolute differences between the coordinates of two points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b2c877-db40-4ddf-aa37-79a862094fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4:\n",
    "# some common parametrs are:\n",
    "# 1.Number of Neighbors (k): The number of neighbors to consider is a crucial hyperparameter in KNN.\n",
    "# 2.Distance Metric: The choice of distance metric, as discussed earlier, is an important hyperparameter.\n",
    "# 3.Data Preprocessing: KNN classifiers and regressors can be sensitive to the scale and distribution of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "865892f9-7dd8-4ce0-a483-d66dc9fdef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5:\n",
    "# The size of the training set in a KNN classifier or regressor can significantly impact the model's performance.\n",
    "# 1.Overfitting and Underfitting: KNN models can suffer from both overfitting and underfitting depending on the size of the training set.\n",
    "# When the training set is too small, the model may have a limited representation of the underlying data distribution, leading to overfitting.\n",
    "# 2.Bias-Variance Tradeoff: The size of the training set also affects the bias-variance tradeoff. \n",
    "#   With a small training set, the model tends to have high variance as it is more sensitive to individual data points.\n",
    "\n",
    "# Techniques used to optimise the size of the training set\n",
    "# 1.Learning Curves: Learning curves can provide insights into the relationship between the training set size and the model's performance.\n",
    "# 2.Data Sampling: If the training set is excessively large and computational resources are limited, \n",
    "#   sampling techniques can be used to reduce its size while maintaining representative information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020750c-04cd-4f8e-bd6f-71eaaa853a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6:\n",
    "# 1.Computational Complexity: One of the main challenges with KNN is its computational complexity, \n",
    "#  especially when dealing with large datasets or high-dimensional feature spaces.\n",
    "# 2.Sensitivity to Feature Scaling: KNN is sensitive to the scale and distribution of features.\n",
    "#   If the features have different scales, those with larger magnitudes can dominate the distance calculation and influence the results.\n",
    "# 3.Imbalanced Data: KNN can struggle with imbalanced datasets, where the number of instances in different classes is significantly different.\n",
    "\n",
    "# we can overcome these drawbacks\n",
    "# 1.Dimensionality Reduction: Applying dimensionality reduction techniques, such as Principal Component Analysis (PCA), \n",
    "#   can reduce the feature space's dimensionality while retaining important information.\n",
    "# 2.Feature Scaling: Normalize or standardize the features so that they have similar scales. \n",
    "# 3.Grid Search and Cross-Validation: Employ techniques like grid search and cross-validation to explore different \n",
    "#   combinations of hyperparameter values and evaluate the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
